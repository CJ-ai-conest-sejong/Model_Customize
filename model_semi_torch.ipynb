{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_semi_torch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtYfGWEuDdxj"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3jRzczNDphd"
      },
      "source": [
        "# 오..이걸 torch로..(박수박수)n",
        "# user = torch.tensor([[10]]) #=>만약, user를 특정할 수 있는 정보가 있다면, 이 input을 하나 더 사용하면 좋다.\n",
        "hj = torch.tensor([10])\n",
        "sex = torch.tensor([10])\n",
        "age = torch.tensor([10])\n",
        "item_buz = torch.tensor([10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSQJz2rvF-tH"
      },
      "source": [
        "class Model_semi_torch(nn.Module):\n",
        "    def __init__(self,hidden_size,lin_size, embedding_matrix=embedding_matrix):\n",
        "        super(Model_semi_torch, self).__init__()\n",
        "\n",
        "        # Initialize some parameters for your model\n",
        "        self.hidden_size = hidden_size\n",
        "        # drp = 0.1\n",
        "\n",
        "        K = 200\n",
        "        # U_embedding = nn.Embedding(2000,K)(unit) #user를 특정 할 수 있는 정보가 있을 때 사용\n",
        "        self.H_embedding = nn.Embedding(H, K)\n",
        "        self.S_embedding = nn.Embedding(S, K)\n",
        "        self.A_embedding = nn.Embedding(A, K)\n",
        "        self.I_embedding = nn.Embedding(I, K)\n",
        "\n",
        "        #user_bias = Embedding(U.max()+1,1,embeddings_regularizer=l2())(user) #user를 특정 할 수 있는 정보가 있을 때 사용\n",
        "        self.H_bias = nn.Embedding(H,1)\n",
        "        self.S_bias = nn.Embedding(S,1)\n",
        "        self.A_bias = nn.Embedding(A,1)\n",
        "        self.I_bias = nn.Embedding(I,1)\n",
        "        \n",
        "        # self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        # self.embedding.weight.requires_grad = False\n",
        "        # self.embedding_dropout = nn.Dropout2d(0.1)\n",
        "\n",
        "\n",
        "        self.linear1 = nn.Linear(, 2048)\n",
        "        self.linear2 = nn.Linear(2048, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # self.dropout = nn.Dropout(drp)\n",
        "\n",
        "        self.out = nn.Linear(256, 1)\n",
        "\n",
        "        # torch.flatten\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x : Concatenated input (hj, sex, age ,item_buz)\n",
        "        '''\n",
        "        h_embedding = torch.flatten(self.H_embedding(x[0]), 1)\n",
        "        # h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
        "        s_embedding = torch.flatten(self.S_embedding(x[1]), 1)\n",
        "        # s_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(s_embedding, 0)))\n",
        "        a_embedding = torch.flatten(self.A_embedding(x[2]), 1)\n",
        "        # a_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(a_embedding, 0)))\n",
        "        i_embedding = torch.flatten(self.I_embedding(x[3]), 1)\n",
        "        # i_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(i_embedding, 0)))\n",
        "\n",
        "        h_bias = torch.flatten(self.H_bias(x[0]), 1)\n",
        "        # h_bias = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_bias, 0)))\n",
        "        s_bias = torch.flatten(self.S_bias(x[1]), 1)\n",
        "        # s_bias = torch.squeeze(self.embedding_dropout(torch.unsqueeze(s_bias, 0)))\n",
        "        a_bias = torch.flatten(self.A_bias(x[2]), 1)\n",
        "        # a_bias = torch.squeeze(self.embedding_dropout(torch.unsqueeze(a_bias, 0)))\n",
        "        i_bias = torch.flatten(self.I_bias(x[3]), 1)\n",
        "        # i_bias = torch.squeeze(self.embedding_dropout(torch.unsqueeze(i_bias, 0)))\n",
        "\n",
        "        R = torch.cat((S_embedding,A_embedding,H_embedding,I_embedding,s_bias,a_bias,h_bias,i_bias), -1)\n",
        "\n",
        "        R = self.relu(self.linear1(R))\n",
        "        R = self.relu(self.linear2(R))\n",
        "        out = self.out(R)\n",
        "        # return the final output\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
